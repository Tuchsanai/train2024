{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAaob1CB7WIG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "de990c40325748f7b529eeca245f4420",
      "22e160bb2e114e04a6c2bfb549335aab",
      "4c0d9112bee44e6eb1efc548f140eded",
      "2bc41ef116e04b508ee927a4dec4f8b6",
      "8dd7d12e13c645c58e5576387cb7cea8",
      "7476717c445047fb89c584e71c9937c0",
      "a95ed62f06b8440687cb0f43e844745c",
      "5aaa873d86ec4456aab3fd4f344d2d85"
     ]
    },
    "colab_type": "code",
    "id": "EkgWCY2m7WIM",
    "outputId": "e65f87c8-046c-4d0f-8b19-aeab5097e3be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to ./cifar10.tgz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de990c40325748f7b529eeca245f4420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the Dataset\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url,root='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9mdYdhLl7WIQ",
    "outputId": "8e906ba5-1147-49b9-b3e0-116cc80e5b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /raid/scratch/$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R /raid/scratch/data /raid/scratch/$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ukf2Pgqt7WIT",
    "outputId": "52a33ef1-168d-446c-b30f-60eda057255b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'test']\n",
      "['airplane', 'automobile', 'dog', 'truck', 'bird', 'ship', 'horse', 'deer', 'frog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/raid/scratch/user03/data/cifar10'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + '/train')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43OmVhya7WIX"
   },
   "source": [
    "The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the ImageFolder class from torchvision to load the data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysSGHeyt7WIY"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGnQNKhS7WIb"
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(data_dir + '/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_TWJ3tc7WIf"
   },
   "source": [
    "Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of 32x32 px color images with 3 channels (RGB), each image tensor has the shape ```(3, 32, 32)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "rin9CiCz7WIg",
    "outputId": "a775eb4e-2bfe-4272-983b-eb0655104620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n",
       "         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n",
       "         ...,\n",
       "         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n",
       "         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n",
       "         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n",
       "\n",
       "        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n",
       "         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n",
       "         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n",
       "         ...,\n",
       "         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n",
       "         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n",
       "         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n",
       "\n",
       "        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n",
       "         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n",
       "         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n",
       "         ...,\n",
       "         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n",
       "         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n",
       "         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = dataset[0]\n",
    "print(img.shape, label)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0LzJL8u7WIk"
   },
   "source": [
    "The list of classes is stored in the ```.classes``` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ItDw43e-7WIl",
    "outputId": "28af9cf2-0f4c-4de0-969a-b0429b7767ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "HT4f55Cl7WIo",
    "outputId": "62a8e663-f7f8-402c-a7a7-a5fb8c18c545"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for img in dataset[1]:\n",
    "    plt.imshow(img.permute((1,2,0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c1350Dyd7WIr",
    "outputId": "46bf0e20-5c2f-4b3a-d097-82c585b4bcd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-kEcvDH7WIv"
   },
   "source": [
    "### Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W_QcXetu7WIv",
    "outputId": "6fbb31fa-c0b3-4755-f5c6-dd4112fd4edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45001, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset,[train_size,val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rGa-pN27WI0"
   },
   "source": [
    "We can now create data loaders for training and validation, to load the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCDL61IG7WI0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "val_dl = DataLoader(val_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "#pin_memory (bool, optional) – If True, the data loader will copy Tensors into CUDA pinned memory before returning them.\n",
    "#num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZTQS0q-7WI3"
   },
   "source": [
    "## Defining the Model (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0412_jL7WI4"
   },
   "source": [
    "The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n",
    "\n",
    "![conv2d](https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
    "\n",
    "Let us implement a convolution operation on a 1 channel image with a 3x3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UfkxAZM7WI5"
   },
   "outputs": [],
   "source": [
    "def apply_kernel(image,kernel):\n",
    "    ri, ci = image.shape # image dimensions\n",
    "    rk, ck = kernel.shape # kernel dimensions\n",
    "    ro, co = ri-rk+1, ci-ck+1 # output dimensions\n",
    "    output = torch.zeros([ro,co])\n",
    "    for i in range(ro):\n",
    "        for j in range(co):\n",
    "            output[i,j] = torch.sum(image[i:i+rk, j:j+ck] * kernel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "85pdbMQ17WI9",
    "outputId": "e718981d-322c-45f9-db23-252bf5b16ec1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 17.],\n",
       "        [10., 17., 19.],\n",
       "        [ 9.,  6., 14.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = torch.tensor([\n",
    "    [3, 3, 2, 1, 0], \n",
    "    [0, 0, 1, 3, 1], \n",
    "    [3, 1, 2, 2, 3], \n",
    "    [2, 0, 0, 2, 2], \n",
    "    [2, 0, 0, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "sample_kernel = torch.tensor([\n",
    "    [0, 1, 2], \n",
    "    [2, 2, 0], \n",
    "    [0, 1, 2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "apply_kernel(sample_image,sample_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X30SzXIg7WJC"
   },
   "source": [
    "For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise.\n",
    "\n",
    "There are certain advantages offered by convolutional layers when working with image data:\n",
    "\n",
    "* Fewer parameters: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer.\n",
    "\n",
    "* Sparsity of connections: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n",
    "\n",
    "* Parameter sharing and spatial invariance: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n",
    "\n",
    "We will also use a max-pooling layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n",
    "\n",
    "![max-pooling](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvP4-cP-7WJD"
   },
   "source": [
    "The Conv2d layer transforms a 3-channel image to a 16-channel feature map, and the MaxPool2d layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n",
    "\n",
    "![cnn](https://i.imgur.com/KKtPOKE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e908yuwY7WJD"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _,preds = torch.max(outputs,dim=1)\n",
    "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5N3wduc77WJG"
   },
   "outputs": [],
   "source": [
    "# class Cifar10CnnModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#         nn.Conv2d(3,32,kernel_size=3,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2,2),# output: 64 x 16 x 16-> using h1=((ho - F +2P)/s) + 1)\n",
    "        \n",
    "#         nn.Conv2d(64,128,kernel_size=3, stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(128,128,kernel_size=3, stride=1,padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2, 2), # output: 128 x 8 x 8-> using h1=((ho - F +2P)/s) + 1)\n",
    "        \n",
    "#         nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.MaxPool2d(2, 2),# output: 256 x 4 x 4 -> using h1=((ho - F +2P)/s) + 1)\n",
    "\n",
    "#         nn.Flatten(),\n",
    "#         nn.Linear(256*4*4,1024),# using h1=((ho - F +2P)/s) + 1)\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(1024,512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(512,10))\n",
    "    \n",
    "#     def forward(self,xb):\n",
    "#         return self.network(xb)\n",
    "    \n",
    "#     def training_step(self,batch):\n",
    "#         images,labels = batch\n",
    "#         out = self(images)\n",
    "#         loss = F.cross_entropy(out,labels)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self,batch):\n",
    "#         images,labels = batch\n",
    "#         out = self(images)\n",
    "#         loss = F.cross_entropy(out,labels)\n",
    "#         acc = accuracy(out,labels)\n",
    "#         return {'val_loss':loss,'val_acc':acc}\n",
    "    \n",
    "#     def validation_epoch_end(self,outputs):\n",
    "#         batch_losses = [x['val_loss'] for x in outputs]\n",
    "#         epoch_loss = torch.stack(batch_losses).mean()\n",
    "#         batch_acc = [x['val_acc'] for x in outputs]\n",
    "#         epoch_acc = torch.stack(batch_acc).mean()\n",
    "#         return {'val_loss':epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "#     def epoch_end(self,epoch,result):\n",
    "#         print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fAj1_xL7WJK"
   },
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}] val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "        \n",
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hrVOkPq7WJN"
   },
   "outputs": [],
   "source": [
    "model = Cifar10CnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "Th0-ClsQ7WJP",
    "outputId": "514b4015-26fe-4285-8bfc-7b6677b297f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Cifar10CnnModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRzZyFNr7WJS"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NkyJxg4g7WJV",
    "outputId": "cf6f0433-a30e-442d-8b1e-24f8d2010119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLDpGG6H7WJY"
   },
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if isinstance(data, (list,tuple)): #The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iy-r3frt7WJb"
   },
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAo2ANed7WJf"
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_c1BC1D7WJj"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "def fit(epochs,lr,model,train_loder,val_loader,opt_func = torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loder:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()#used to update the parameters\n",
    "            optimizer.zero_grad()#Clears the gradients of  optimizer\n",
    "        result = evaluate(model,val_loader)\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_aPvG3q7WJm"
   },
   "outputs": [],
   "source": [
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YPZZwStB7WJp",
    "outputId": "d1cb56ea-e7ed-427b-d0f6-e2dc2b95f571"
   },
   "outputs": [],
   "source": [
    "evaluate(model,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "c6M71sWm7WJr",
    "outputId": "090e7451-00a9-48e2-ce30-8a305669e907"
   },
   "outputs": [],
   "source": [
    "history = fit(10,0.001,model,train_dl,val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "v6ATPUMr7WJu",
    "outputId": "e2eb6bdf-4f36-48de-ae1f-084155a441d7"
   },
   "outputs": [],
   "source": [
    "accuracies = [x['val_acc'] for x in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "u5i7Onlv80kV",
    "outputId": "8cb51cc3-f089-4a53-ae71-64097be184bf"
   },
   "outputs": [],
   "source": [
    "val_losses = [x['val_loss'] for x in history]\n",
    "plt.plot(val_losses, '-rx')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSbvL8-D8_HY"
   },
   "outputs": [],
   "source": [
    "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNEbsEGn9Kh3"
   },
   "outputs": [],
   "source": [
    "def predict_img(img,model):\n",
    "    xb = to_device(img.unsqueeze(0),device)\n",
    "    yb = model(xb)\n",
    "    _,preds = torch.max(yb,dim=1)\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "WOxAytot9os-",
    "outputId": "8778fa29-ac8f-4ab3-ae25-8e75ee882c7c"
   },
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "3dmmYueJ9sqY",
    "outputId": "419139e5-bb4a-4468-c9e9-ae42b1be0f13"
   },
   "outputs": [],
   "source": [
    "img, label = test_dataset[1002]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tvt58MTX907s",
    "outputId": "61dfaff9-4568-4d5c-c5e1-afe42c6a0c54"
   },
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeYArTot95FT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Classification using CNN in Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "22e160bb2e114e04a6c2bfb549335aab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bc41ef116e04b508ee927a4dec4f8b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5aaa873d86ec4456aab3fd4f344d2d85",
      "placeholder": "​",
      "style": "IPY_MODEL_a95ed62f06b8440687cb0f43e844745c",
      "value": " 135110656/? [00:06&lt;00:00, 20055171.30it/s]"
     }
    },
    "4c0d9112bee44e6eb1efc548f140eded": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7476717c445047fb89c584e71c9937c0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8dd7d12e13c645c58e5576387cb7cea8",
      "value": 1
     }
    },
    "5aaa873d86ec4456aab3fd4f344d2d85": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7476717c445047fb89c584e71c9937c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dd7d12e13c645c58e5576387cb7cea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a95ed62f06b8440687cb0f43e844745c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de990c40325748f7b529eeca245f4420": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c0d9112bee44e6eb1efc548f140eded",
       "IPY_MODEL_2bc41ef116e04b508ee927a4dec4f8b6"
      ],
      "layout": "IPY_MODEL_22e160bb2e114e04a6c2bfb549335aab"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
