{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /raid/scratch/$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R /raid/scratch/data /raid/scratch/$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MNIST(root = 'data/',train=True,transform=ToTensor())\n",
    "dataset = MNIST(root = '/raid/scratch/user03/data/',train=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 10000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds,val_ds = random_split(dataset,[train_size,val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:562: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_loder = DataLoader(train_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
    "val_loader = DataLoader(val_ds,batch_size,shuffle=True,num_workers=4,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a batch of data in a grid using the ```make_grid``` function from ```torchvision```. We'll also use the ```.permute``` method on the tensor to move the channels to the last dimension, as expected by ```matplotlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1041, in launch_instance\\n    app.start()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 724, in start\\n    self.io_loop.start()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\\n    self._run_once()\\n', '  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\\n    handle._run()\\n', '  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 512, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 501, in process_one\\n    await dispatch(*args)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 408, in dispatch_shell\\n    await result\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 731, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 417, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\\n    result = self._run_cell(\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\\n    return runner(coro)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_233243/101250134.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/__init__.py\", line 944, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 190, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 188, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:253\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:136\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 136\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:368\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:386\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images,_ \u001b[38;5;129;01min\u001b[39;00m train_loder:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages Shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:443\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:389\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1056\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     current_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mcurrent_device()  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1056\u001b[0m     current_device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# choose cuda for default\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m pin_memory_thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(\n\u001b[1;32m   1058\u001b[0m     target\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39m_pin_memory_loop,\n\u001b[1;32m   1059\u001b[0m     args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_result_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue,\n\u001b[1;32m   1060\u001b[0m           current_device,\n\u001b[1;32m   1061\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread_done_event, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device))\n\u001b[1;32m   1062\u001b[0m pin_memory_thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:563\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    562\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:257\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    255\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00morig_traceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n', '  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1041, in launch_instance\\n    app.start()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 724, in start\\n    self.io_loop.start()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\\n    self._run_once()\\n', '  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\\n    handle._run()\\n', '  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 512, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 501, in process_one\\n    await dispatch(*args)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 408, in dispatch_shell\\n    await result\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 731, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 417, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 540, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\\n    result = self._run_cell(\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\\n    return runner(coro)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_233243/101250134.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/__init__.py\", line 944, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 190, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 188, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']"
     ]
    }
   ],
   "source": [
    "for images,_ in train_loder:\n",
    "    print('Images Shape:', images.shape)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(make_grid(images,nrow=16).permute((1,2,0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "To improve upon logistic regression, we'll create a neural network with one hidden layer. Here's what this means:\n",
    "\n",
    "* Instead of using a single nn.Linear object to transform a batch of inputs (pixel intensities) into a batch of outputs (class probabilities), we'll use two nn.Linear objects. Each of these is called a layer in the network.\n",
    "\n",
    "* The first layer (also known as the hidden layer) will transform the input matrix of shape batch_size x 784 into an intermediate output matrix of shape batch_size x hidden_size, where hidden_size is a preconfigured parameter (e.g. 32 or 64).\n",
    "\n",
    "* The intermediate outputs are then passed into a non-linear activation function, which operates on individual elements of the output matrix.\n",
    "\n",
    "* The result of the activation function, which is also of size batch_size x hidden_size, is passed into the second layer (also knowns as the output layer), which transforms it into a matrix of size batch_size x 10, identical to the output of the logistic regression model.\n",
    "\n",
    "Introducing a hidden layer and an activation function allows the model to learn more complex, multi-layered and non-linear relationships between the inputs and the targets. Here's what it looks like visually:\n",
    "\n",
    "![image](https://i.imgur.com/vDOGEkG.png)\n",
    "\n",
    "The activation function we'll use here is called a ***Rectified Linear Unit or ReLU***, and it has a really simple formula: ```relu(x) = max(0,x)``` i.e. if an element is negative, we replace it by 0, otherwise we leave it unchanged.\n",
    "\n",
    "To define the model, we extend the ```nn.Module``` class, just as we did with logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _,preds = torch.max(outputs,dim = 1) ## _ here max prob will come and we don't require it now\n",
    "    return torch.tensor(torch.sum(preds == labels).item()/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,out_size)\n",
    "    def forward(self,xb):\n",
    "        xb = xb.view(xb.size(0),-1) ## same as .reshape()\n",
    "        out = self.linear1(xb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        acc = accuracy(out,labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self,outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_acc = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        return {'val_loss': epoch_loss.item(),'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "model = MnistModel(input_size,hidden_size = 32,out_size = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a GPU is available and the required NVIDIA CUDA drivers are installed using ```torch.cuda.is_available```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a helper function to ensure that our code uses the GPU if available, and defaults to using the CPU if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function that can move data and model to a chosen device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if isinstance(data, (list,tuple)): #The isinstance() function returns True if the specified object is of the specified type, otherwise False.\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a DeviceDataLoader class to wrap our existing data loaders and move data to the selected device, as a batches are accessed. Interestingly, we don't need to extend an existing class to create a PyTorch dataloader. All we need is an __iter__ method to retrieve batches of data, and an __len__ method to get the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b,self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```yield``` statement suspends function’s execution and sends a value back to the caller,\n",
    "but retains enough state to enable function to resume where it is left off.\n",
    "When resumed, the function continues execution immediately after the last yield run.\n",
    "This allows its code to produce a series of values over time,rather than computing them at once and sending them back \n",
    "like a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now wrap our data loaders using ```DeviceDataLoader```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loder = DeviceDataLoader(train_loder,device)\n",
    "val_loader = DeviceDataLoader(val_loader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,train_loder,val_loader,opt_func = torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loder:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()#used to update the parameters\n",
    "            optimizer.zero_grad()#Clears the gradients of  optimizer\n",
    "        result = evaluate(model,val_loader)\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel(input_size, hidden_size=32, out_size=num_classes)\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(15,0.5,model,train_loder,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [x['val_loss'] for x in history]\n",
    "plt.plot(losses, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [x['val_acc'] for x in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
